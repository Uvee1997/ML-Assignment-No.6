{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0afc47",
   "metadata": {},
   "source": [
    "1) In the sense of machine learning, what is a model? What is the best way to train a model?\n",
    "\n",
    ":- The model artifact that is created by the training process.\n",
    "   Step 1: Begin with existing data. Machine learning requires us to have existing data—not the data our application will              use when we run it, but data to learn from.\n",
    "   Step 2: Analyze data to identify patterns.\n",
    "   Step 3: Make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186002d1",
   "metadata": {},
   "source": [
    "2) In the sense of machine learning, explain the \"No Free Lunch\" theorem.\n",
    "\n",
    ":- The theory tells that when the performance of all optimization methods is averaged across all conceivable problems,        they all perform equally well. It indicates that no one optimum optimization algorithm exists. Because of the strong        link between optimization, search, and machine learning, there is no one optimum machine learning method for predictive    modelling tasks like classification and regression.\n",
    "   They all agree on one point: there is no “best” algorithm for specific kinds of algorithms, since they all perform          similarly on average. Mathematically, the computing cost of finding a solution is the same for any solution technique      when averaged across all problems in the class. As a result, no solution provides a shortcut. \n",
    "   There are 2 types of No Free Lunch theorem (NFL):- i)machine learning ii) search & optimization\n",
    "   \n",
    "   According to the “No Free Lunch” theory, there is no one model that works best for every situation. Because the            assumptions of a great model for one issue may not hold true for another, it is typical in machine learning to attempt      many models to discover the one that performs best for a specific problem. This is especially true in supervised            learning, where validation or cross-validation is frequently used to compare the prediction accuracy of many models of      various complexity in order to select the optimal model. A good model may also be trained using several methods — for      example, linear regression can be learned using normal equations or gradient descent.\n",
    "   \n",
    "   According to the “No Free Lunch” theorem, all optimization methods perform equally well when averaged over all              optimization tasks without re-sampling. This fundamental theoretical notion has had the greatest impact on optimization,    search, and supervised learning. The first theorem, No Free Lunch, was rapidly formulated, resulting in a series of        research works, which defined a whole field of study with meaningful outcomes across different disciplines of science      where the effective exploration of a search region is a vital and crucial activity.\n",
    "\n",
    "   In general, its usefulness is as important as the algorithm. An effective solution is created by matching the utility      with the algorithm. If no good conditions for the objective function are known, and one is just working with a black        box, no guarantee can be made that this or that method outperforms a (pseudo)random search.A framework is being created    to investigate the relationship between successful optimization algorithms and the issues they solve. A series of “no      free lunch” (NFL) theorems are provided, establishing that any improved performance over one class of tasks is              compensated by improved performance over another. These theorems provide a geometric explanation of what it means for an    algorithm to be well matched to an optimization issue.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6271aaa",
   "metadata": {},
   "source": [
    "3) Describe the K-fold cross-validation mechanism in detail.\n",
    "\n",
    ":- In each set (fold) training and the test would be performed precisely once during this entire process. It helps us to      avoid overfitting. As we know when a model is trained using all of the data in a single short and give the best            performance accuracy. To resist this k-fold cross-validation helps us to build the model is a generalized one.\n",
    "\n",
    "   To achieve this K-Fold Cross Validation, we have to split the data set into three sets, Training, Testing, and              Validation, with the challenge of the volume of the data.Here Test and Train data set will support building model and      hyperparameter assessments.\n",
    "   In which the model has been validated multiple times based on the value assigned as a parameter and which is called K      and it should be an INTEGER.\n",
    "   Make it simple, based on the K value, the data set would be divided, and train/testing will be conducted in a sequence      way equal to K time.\n",
    "   In which each data point is used, once in the hold-out set and K-1 in Training. So, during the full iteration at least      once, one fold will be used for testing and the rest for training.\n",
    "   In the above set, 5- Testing 20 Training. In each iteration, we will get an accuracy score and have to sum them and find    the mean. Here we can understand how the data is spread in a way of consistency and will make a conclusion whether to      for the production with this model (or) NOT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46ca58",
   "metadata": {},
   "source": [
    "4) Describe the bootstrap sampling method. What is the aim of it?\n",
    "\n",
    ":- The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with      replacement. It can be used to estimate summary statistics such as the mean or standard deviation.\n",
    "   Aim - estimate statistics on a population by sampling a dataset with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26376c43",
   "metadata": {},
   "source": [
    "5) What is the significance of calculating the Kappa value for a classification model? Demonstrate how to measure the Kappa    value of a classification model using a sample collection of results.\n",
    "\n",
    ":- It tells you how much better your classifier is performing over the performance of a classifier that simply guesses at      random according to the frequency of each class. It can also be used as performance of a classification model.\n",
    "   Let’s focus on a classification task on bank loans, using the German credit data provided by the UCI Machine Learning      Repository. In this dataset, bank customers have been assigned either a “bad” credit rating (30%) or a “good” credit        rating (70%) according to the criteria of the bank. For the purpose of this article, we exaggerated the imbalance in the    target class credit rating via bootstrapping, giving us 10% with a “bad” credit rating and 90% with a “good” credit        rating: a highly imbalanced dataset. Exaggerating the imbalance helps us to make the difference between “overall            accuracy” and “Cohen’s kappa” clearer in this article.\n",
    "\n",
    "Let’s partition the data into a training set (70%) and a test set (30%) using stratified sampling on the target column and then train a simple model, e.g., a decision tree. Given the high imbalance between the two classes, the model will not perform very well. Nevertheless, let’s use its performance as the baseline for this study.\n",
    "\n",
    "Figure 1 shows the confusion matrix and accuracy statistics for this baseline model. The overall accuracy of the model is quite high (87%) and hints at an acceptable performance by the model. However, in the confusion matrix, we can see that the model is able to classify only nine out of the 30 credit customers with a bad credit rating correctly. This is also visible by the low sensitivity value of class “bad” — just 30%. Basically, the decision tree is classifying most of the “good” customers correctly and neglecting the necessary performance on the few “bad” customers. The imbalance in the class a priori probability compensates for such sloppiness in classification. Let’s note for now that the Cohen’s kappa value is just 0.244, within its range of [-1,+1]. Let’s try to improve the model performance by forcing it to acknowledge the existence of the minority class. We train the same model this time on a training set where the minority class has been oversampled using the SMOTE technique, reaching a class proportion of 50% for both classes.\n",
    "\n",
    "To provide more detail about the confusion matrix for this model, 18 out of the 30 customers with a “bad” credit rating are detected by the model, leading to a new sensitivity value of 60%. Cohen’s kappa statistics is now 0.452 for this model, which is a remarkable increase from the previous value 0.244. But what about overall accuracy? For this second model, it’s 89%, not very different from the previous value of 87%.\n",
    "\n",
    "When summarizing, we get two very different pictures. According to the overall accuracy, model performance hasn’t changed very much at all.\n",
    " Formula for kappa values :- \n",
    "      k = Po - Pe/1-Pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d24ca",
   "metadata": {},
   "source": [
    "6) Describe the model ensemble method. In machine learning, what part does it play?\n",
    "\n",
    ":- Ensemble method is process where multiple diverse models are created to predict an outcome, either by using many            different modeling algorithms or using different training data sets. It basically used for predicting an outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4befc0a1",
   "metadata": {},
   "source": [
    "7) What is a descriptive model's main purpose? Give examples of real-world problems that descriptive models were used to      solve.\n",
    "\n",
    ":- A descriptive model describes a system or other entity and its relationship to its environment. It basically used to        specify or understand what the system is, what it does & how it does it.\n",
    "   It include KPI's year-on-year percentage sales growth, revenue per customer and the average time customers take to pay      bills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8d11d",
   "metadata": {},
   "source": [
    "8) Describe how to evaluate a linear regression model.\n",
    "\n",
    ":- i) Show there is relation between 2 variables\n",
    "   ii) Independent feature will should have normal distribution\n",
    "   iii) Always take care of multicoilnearty\n",
    "   iv) Homoscedestity = same variance(satisfactory model)\n",
    "       Heteoscedasity = It refers to situation where the variance of the residual is unequal over a range of measured                               values. (unsatisfactory model).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd573d",
   "metadata": {},
   "source": [
    "9) Distinguish :\n",
    "    \n",
    "   i)Descriptive vs. predictive models\n",
    "   - Models that are primarily used for understanding, predicting and communicating are referred to as descriptive models,      whereas models mainly used for implementation are called prescriptive models.\n",
    "   \n",
    "   ii)Underfitting vs. overfitting the model\n",
    "   - Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid      the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may        not learn enough from the training data.\n",
    "     Underfitting model is high bias & low variance, while overfitting is low bias & high variance.\n",
    "     \n",
    "   iii)Bootstrapping vs. cross-validation\n",
    "     - Cross validation splits the available dataset to create multiple datasets, and Bootstrapping method uses the                original dataset to create multiple datasets after resampling with replacement. When doing model validation                bootstrapping is weak, while cross validation is strong        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5aaa25",
   "metadata": {},
   "source": [
    "10) Make quick notes on:\n",
    "    \n",
    "    i) LOOCV - (Leave One Out Cross-Validation)\n",
    "       It is a type of cross-validation approach in which each observation is considered as the validation set and the rest        (N-1) observations are considered as the training set.In LOOCV, fitting of the model is done and predicting using          one observation validation set.\n",
    "       \n",
    "    ii) F-measurement\n",
    "        It is also called as F-score.The F measure (F1 score or F score) is a measure of a test's accuracy and is defined           as the weighted harmonic mean of the precision and recall of the test.The F1 score is the harmonic mean of the             precision and recall. The more generic {\\displaystyle F_{\\beta }}F_{\\beta } score applies additional weights,               valuing one of precision or recall more than the other.\n",
    "        The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible           value is 0, if either the precision or the recall is zero. The F1 score is also known as the Sørensen–Dice                 coefficient or Dice similarity coefficient (DSC).\n",
    "     \n",
    "    iii) The width of the silhouette\n",
    "         The Average Silhouette Width (ASW) of a clustering is ̄ a ( i ) is the average distance of to points in the cluster          to which it was assigned, and is the average distance of to the points in the nearest cluster to which it was not          assigned.The silhouette value describes how similar a gene is to its own cluster (cohesion) compared to other              clusters (separation). A high value indicates that the gene is well placed. So if the average of all of these              silhouettes is high then the number of clusters is good.\n",
    "      \n",
    "     iv) Receiver operating characteristic curve(ROC)\n",
    "         A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic                ability of a binary classifier system as its discrimination threshold is varied. The method was originally                  developed for operators of military radar receivers. The ROC curve is created by plotting the true positive rate            (TPR) against the false positive rate (FPR) at various threshold settings.  \n",
    "          The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting             enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of                 stimuli.  \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
